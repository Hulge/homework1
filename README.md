# 大数据用户分析项目

本项目旨在针对10GB和30GB两组不同规模的数据集，进行用户画像构建与潜在高价值用户挖掘。项目采用模块化结构组织，分别针对不同数据集设计了完整的数据处理与分析流程。

## 项目结构

```
code/
│
├── 10G_data/
│   ├── 10G_3.1.py   # 探索性分析与可视化
│   ├── 10G_3.2.py   # 数据预处理
│   └── 10G_3.3.py   # 潜在高价值用户识别
│
└── 30G_data/
    ├── 30G_3.1.py   # 探索性分析与可视化
    ├── 30G_3.2.py   # 数据预处理
    └── 30G_3.3.py   # 潜在高价值用户识别
```

## 文件说明

### 10G_data

- **10G_3.1.py**  
  对10GB数据集进行初步的探索性分析和多种可视化，包括收入分布、商品类别分布、国家用户分布、消费金额分布等。

- **10G_3.2.py**  
  对10GB数据集进行数据质量检查与预处理。主要包括缺失值处理、时间字段逻辑校验、异常标注与统计分析。

- **10G_3.3.py**  
  采用MiniBatchKMeans算法进行聚类，识别潜在的高价值用户群体，输出聚类中心特征并保存每个聚类簇的成员列表。

### 30G_data

- **30G_3.1.py**  
  对30GB数据集进行探索性数据分析（EDA）与可视化，分析方法与10G数据集一致，但适配更大规模数据的处理策略。

- **30G_3.2.py**  
  针对30GB数据集进行系统性数据预处理，包括格式标准化、缺失值与异常值修正。

- **30G_3.3.py**  
  对30GB数据集执行潜在高价值用户的聚类与分析，同样使用MiniBatchKMeans方法，并针对超大数据量进行了批次优化。

## 技术栈

- **Python 3.x**
- **PyArrow** （读取大型Parquet文件）
- **Pandas** （数据处理）
- **Matplotlib**、**Seaborn** （数据可视化）
- **Scikit-learn** （聚类建模）

## 使用方法

1. 将10GB或30GB数据集分别放置在对应路径下（如：`data/raw data/10G_data_new` 或 `data/raw data/30G_data_new`）。
2. 依次运行各模块脚本，完成数据分析与处理流程：
   - 第一步：运行 `3.1.py` 脚本进行数据探索与可视化；
   - 第二步：运行 `3.2.py` 脚本进行数据预处理；
   - 第三步：运行 `3.3.py` 脚本进行潜在高价值用户识别。
3. 结果包括：
   - 多种数据可视化图表
   - 清洗后的数据文件
   - 聚类分析输出及用户标签结果

## 注意事项

- 项目中大量数据处理采用了**分批加载**、**增量计算**等策略，确保在普通内存环境下也能流畅运行。
- 对于30GB数据集，建议确保机器内存 ≥ 16GB，避免内存溢出问题。
- 推荐使用具备中文字体支持的环境，以正确显示可视化图表中的中文内容。
